---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
spec:
  interval: 30m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.16.5
      sourceRef:
        kind: HelmRepository
        name: rook-release
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    monitoring:
      enabled: true
      createPrometheusRules: true
    cephClusterSpec:
      mon:
        count: 3
        allowMultiplePerNode: true # TODO set false after scaling to more nodes
      mgr:
        count: 1 # TODO make it 2 after scaling
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
        prometheusEndpoint:
      storage:
        useAllNodes: false

        osdsPerDevice: 1
      crashCollector:
        disabled: false
        daysToRetain: 14
      network:
        provider: host
      resources:
        osd:
          limits:
            memory: 4Gi
          requests:
            memory: 2Gi
        mon:
          limits:
            memory: 1Gi
          requests:
            memory: 512Mi
        mgr:
          limits:
            memory: 2Gi
          requests:
            memory: 512Mi
    ingress:
      dashboard:
        annotations:
          external-dns.alpha.kubernetes.io/hostname: rook.${SECRET_DOMAIN}
        host:
          name: rook.${SECRET_DOMAIN}
          path: '/'
        ingressClassName: internal
    priorityClassNames:
      mon: system-node-critical
      osd: system-node-critical
      mgr: system-cluster-critical
    cephFileSystems:
      - name: &fsname ceph-filesystem
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 1 # TODO change after scaling
          dataPools:
            - name: replicated
              failureDomain: host
              replicated:
                size: 1 # TODO change after scaling
          metadataServer:
            activeCount: 1 # TODO change after scaling
            activeStandby: false
        storageClass:
          name: *fsname
    toolbox:
      enabled: true
